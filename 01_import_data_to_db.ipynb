{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b54a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from src.get_conn import get_connection_uri\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "db_uri = get_connection_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3096461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "FILES = {\n",
    "    \"games\": \"datasets/games.csv\",\n",
    "    \"players\": \"datasets/players.csv\",\n",
    "    \"plays\": \"datasets/plays.csv\",\n",
    "    \"week_data\": \"datasets/week_data.csv\"  # This may be inside a zip\n",
    "}\n",
    "\n",
    "# Table creation SQL\n",
    "TABLE_SCHEMAS = {\n",
    "    \"games\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS games (\n",
    "            gameId VARCHAR PRIMARY KEY,\n",
    "            gameDate DATE,\n",
    "            gameTimeEastern TIME,\n",
    "            homeTeamAbbr VARCHAR,\n",
    "            visitorTeamAbbr VARCHAR,\n",
    "            week INTEGER\n",
    "        );\n",
    "    \"\"\",\n",
    "    \"players\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS players (\n",
    "            nflId VARCHAR PRIMARY KEY,\n",
    "            height VARCHAR,\n",
    "            weight INTEGER,\n",
    "            birthDate DATE,\n",
    "            collegeName VARCHAR,\n",
    "            position VARCHAR,\n",
    "            displayName VARCHAR\n",
    "        );\n",
    "    \"\"\",\n",
    "    \"plays\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS plays (\n",
    "            gameId VARCHAR,\n",
    "            playId BIGINT,\n",
    "            playDescription TEXT,\n",
    "            quarter INTEGER,\n",
    "            down INTEGER,\n",
    "            yardsToGo INTEGER,\n",
    "            possessionTeam VARCHAR,\n",
    "            playType VARCHAR,\n",
    "            yardlineSide VARCHAR,\n",
    "            yardlineNumber BIGINT,\n",
    "            offenseFormation VARCHAR,\n",
    "            personnelO VARCHAR,\n",
    "            defendersInTheBox BIGINT,\n",
    "            numberOfPassRushers BIGINT,\n",
    "            personnelD VARCHAR,\n",
    "            typeDropback VARCHAR,\n",
    "            preSnapVisitorScore BIGINT,\n",
    "            preSnapHomeScore BIGINT,\n",
    "            gameClock TIME,\n",
    "            absoluteYardlineNumber BIGINT,\n",
    "            penaltyCodes VARCHAR,\n",
    "            penaltyJerseyNumbers VARCHAR,\n",
    "            passResult VARCHAR,\n",
    "            offensePlayResult BIGINT,\n",
    "            playResult BIGINT,\n",
    "            epa FLOAT,\n",
    "            isDefensivePI BOOLEAN\n",
    "        );\n",
    "    \"\"\",\n",
    "    \"week_data\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS week_data (\n",
    "            time TIMESTAMP,\n",
    "            x FLOAT,\n",
    "            y FLOAT,\n",
    "            s FLOAT,\n",
    "            a FLOAT,\n",
    "            dis FLOAT,\n",
    "            o FLOAT,\n",
    "            dir FLOAT,\n",
    "            event VARCHAR,\n",
    "            nflId VARCHAR,\n",
    "            displayName VARCHAR,\n",
    "            jerseyNumber INTEGER,\n",
    "            position VARCHAR,\n",
    "            frameId INTEGER,\n",
    "            team VARCHAR,\n",
    "            gameId VARCHAR,\n",
    "            playId INTEGER,\n",
    "            playDirection VARCHAR,\n",
    "            route VARCHAR\n",
    "        );\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "def read_week_data():\n",
    "    # If week_data.csv exists, read it directly\n",
    "    if os.path.exists(\"datasets/week_data.csv\"):\n",
    "        return pd.read_csv(\"datasets/week_data.csv\")\n",
    "    # Otherwise, look for week_data.zip and extract/read the CSV\n",
    "    elif os.path.exists(\"datasets/week_data.zip\"):\n",
    "        with zipfile.ZipFile(\"datasets/week_data.zip\", \"r\") as z:\n",
    "            with z.open(\"datasets/week_data.csv\") as f:\n",
    "                return pd.read_csv(f)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Neither week_data.csv nor week_data.zip found.\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        conn = psycopg2.connect(db_uri)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Create tables\n",
    "        for table, schema in TABLE_SCHEMAS.items():\n",
    "            cur.execute(schema)\n",
    "            print(f\"Created table: {table}\")\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        # Load and insert data\n",
    "        for table, file_path in FILES.items():\n",
    "            if table == \"week_data\":\n",
    "                df = read_week_data()\n",
    "            else:\n",
    "                df = pd.read_csv(file_path)\n",
    "            df.columns = [col.strip() for col in df.columns]  # Clean column names\n",
    "\n",
    "            # Ensure BIGINT columns are within range and correct dtype\n",
    "            BIGINT_RANGE = (-9223372036854775808, 9223372036854775807)\n",
    "            # Find BIGINT columns for this table\n",
    "            import re\n",
    "            schema = TABLE_SCHEMAS[table]\n",
    "            bigint_cols = [re.findall(r'(\\w+)\\s+BIGINT', line)[0]\n",
    "                        for line in schema.splitlines() if 'BIGINT' in line]\n",
    "            for col in bigint_cols:\n",
    "                if col in df.columns:\n",
    "                    # Convert to numeric, coerce errors, clip to BIGINT range, and convert to int64\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    df[col] = df[col].clip(lower=BIGINT_RANGE[0], upper=BIGINT_RANGE[1])\n",
    "                    df[col] = df[col].astype('Int64')  # Use pandas nullable integer type\n",
    "\n",
    "            # Replace pd.NA and np.nan with None for all columns\n",
    "            df = df.astype(object).where(pd.notnull(df), None)\n",
    "\n",
    "            # Bulk insert data\n",
    "            placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "            columns = ', '.join(df.columns)\n",
    "            sql = f\"INSERT INTO {table} ({columns}) VALUES ({placeholders}) ON CONFLICT DO NOTHING;\"\n",
    "            cur.executemany(sql, df.values.tolist())\n",
    "\n",
    "            print(f\"Inserted data into: {table}\")\n",
    "            conn.commit()\n",
    "\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        print(\"✅ All tables created and data inserted successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
